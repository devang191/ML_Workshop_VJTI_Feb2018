{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from Data\n",
    "\n",
    "In machine learning, most datasets can be represented as tables containing numerical values. Every row is called an **instance**, a **sample**, or a **data point**. Every column is called a **feature** or a **variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call $N$ the number of rows (or number of data samples), and $D$ the number of columns (or number of features). The number $D$ is also called the **dimensionality** of the data. \n",
    "\n",
    "The reason is that we can view this table as a set $E$ of vectors in a space with $D$ dimensions (or **vector space**). \n",
    "\n",
    "Here, a vector $x$ contains $D$ numbers $(x_1, ..., x_D)$, also called **features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [**Supervised learning**](http://en.wikipedia.org/wiki/Supervised_learning) is when we have a label $y$ associated to every data point $x$. The goal is to learn the mapping from $x$ to $y$ from our data. The data gives us this mapping for a finite set of points, but what we want is to **generalize** this mapping. In other words, we want to find the label of any point $x$ that does not belong to our data.\n",
    "\n",
    "* [**Unsupervised learning**](http://en.wikipedia.org/wiki/Unsupervised_learning) is when we don't have any labels. What we want to do is **discover some hidden structure** in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "Mathematically, supervised learning consists of finding a function $f$ that maps a set of points $E$ to a set of labels $F$, knowing a finite set of associations $(x, y)$ which is given by our data. This is what generalization is about: after observing the pairs $(x_i, y_i)$, given a new $x$, we are able to find the corresponding $y$ by applying the function $f$ to $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to split the set of data points into two subsets: the **training set** and the **test set**. We learn the function $f$ on the training set, and test it on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "Broadly speaking, unsupervised learning helps us discover systemic structures in our data. It is harder to grasp than supervised learning, in that there is no precise question and answer in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few important terms related to unsupervised learning: \n",
    "\n",
    "* **Clustering**: Grouping similar points together within clusters.\n",
    "\n",
    "\n",
    "* **Density estimation**: Estimating a probability density that can explain the distribution of the data points.\n",
    "\n",
    "\n",
    "* **Dimension reduction**: Getting a simple representation of high-dimensional data points by projecting them onto a lower-dimensional space. This technique is notably used for data visualization.\n",
    "\n",
    "\n",
    "* **Manifold learning** (or nonlinear dimension reduction): Finding a low-dimensional manifold containing the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and feature extraction\n",
    "\n",
    "In a supervised learning context, when our data contains many features, it is sometimes necessary to choose a subset of them. The features we want to keep are those that are most relevant to our question. This is the problem of **feature selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we may want to extract new features by applying complex transformations on our original dataset. This is **feature extraction**. \n",
    "\n",
    "For example, in computer vision, training a classifier directly on pixels is not the most efficient method in general. We may want to extract the relevant points of interest or make appropriate mathematical transformations. These steps depend on our dataset and on the questions we want to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often necessary to preprocess the data before learning models. **Feature scaling** (or **data normalization**) is a common preprocessing step where features are linearly rescaled to fit in the range $[-1,1]$ or $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, Underfitting, and the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central notion in machine learning is the trade-off between [**overfitting**](http://en.wikipedia.org/wiki/Overfitting) and [**underfitting**](http://en.wikipedia.org/wiki/Underfitting). \n",
    "\n",
    "A model may be able to represent our data accurately. However, if it is **too** accurate, it may not generalize well to unobserved data. For example, in face recognition, a too-accurate model would be unable to identity someone who styled their hair differently that day. The reason is that our model may learn irrelevant features in the training data. \n",
    "\n",
    "On the contrary, an insufficiently trained model would not generalize well either. For example, it would be unable to correctly recognize twins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular solution to **reduce overfitting** consists of adding structure to the model, for example with [**regularization**](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29). This method favors simpler models during training ([Occam's razor](http://en.wikipedia.org/wiki/Occam%27s_razor))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**bias-variance dilemma**](http://en.wikipedia.org/wiki/Bias-variance_dilemma) is closely related. The **bias** of a model quantifies how precise a model is across training sets. The **variance** quantifies how sensitive the model is to small changes in the training set. A robust model is not overly sensitive to small changes. The dilemma involves minimizing both bias and variance; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No model performs uniformly better than the others. One model may perform well on one dataset and badly on another. This is the question of [**model selection**](http://en.wikipedia.org/wiki/Model_selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning : Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Emoji Cheat Sheet**](http://www.emilyinamillion.me/blog/2016/5/30/the-making-of-a-cheatsheet-emoji-edition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now learn the basics of machine learning package [**scikit-learn**](http://scikit-learn.org). \n",
    "\n",
    "\n",
    "- Its clean API makes it really easy to define, train, and test models. \n",
    "\n",
    "\n",
    "- Plus, scikit-learn is specifically designed for speed and (relatively) big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data points should be stored in a $(N,D)$ matrix $X$, where $N$ is the number of observations and $D$ is the number of features. In other words, each row is an observation (data sample). \n",
    "\n",
    "\n",
    "- The first step in a machine learning task is to define what the matrix $X$ is exactly. \n",
    "\n",
    "\n",
    "- In a supervised learning setup, we also have a target, an $N$-long vector $y$ with a scalar value for each observation. This value is either continuous or discrete, depending on whether we have a regression or classifcation problem, respectively.\n",
    "\n",
    "\n",
    "- In scikit-learn, models are implemented in classes that have the **fit()** and **predict()** methods. \n",
    "\n",
    "\n",
    "- The **fit()** method accepts the data matrix $X$ as input, and $y$ as well for supervised learning models. This method trains the model on the given data.\n",
    "\n",
    "\n",
    "- The **predict()** method also takes data points as input (as a $(M,D)$ matrix). It returns the labels or transformed points as predicted by the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember 3 steps in scikit-learn for machine learning:\n",
    "\n",
    "\n",
    "> First, we **create the model** . \n",
    "\n",
    "> Then, we **fit the model to our data**. \n",
    "\n",
    "> Finally, we **predict values** from our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>\n",
    "# Contents\n",
    "## [1. Logistic Regression: Predicting who will survive on the Titanic ](#LogReg)\n",
    "## [2. KNN: Recognizing Handwritten Digits](#KNN)\n",
    "## [3. Naive Bayes: Detecting Insults in Social Commentary (Natural Language Processing)](#NaiveBayes)\n",
    "## [4. Support Vector Machines: Classification](#SVM_C)\n",
    "## [5. Support Vector Machines: Regression](#SVM_R)\n",
    "## [6. Decision Tree Classifier: Evaluating Handwritten Digits](#DecTree)\n",
    "## [7. Dimensionality Reduction: Principal Component Analysis (PCA)](#DR_PCA)\n",
    "## [8. Clustering: Detecting Hidden Structures in the Data (K-Means, Gaussian Mixture Models)](#Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LogReg'></a>\n",
    "# 1. Logistic Regression: Predicting who will survive on the Titanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a [Kaggle competition](http://www.kaggle.com/c/titanic-gettingStarted) where the **goal is to predict survival on the Titanic**, based on real data. \n",
    "\n",
    "[Kaggle](http://www.kaggle.com/competitions) hosts machine learning competitions where anyone can download a dataset, train a model, and test the predictions on the website. The author of the best model wins a price. \n",
    "\n",
    "Here, we use this example to introduce **logistic regression, a basic classifier**. \n",
    "\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- **To predict if a passenger survived the sinking of the Titanic or not.** \n",
    "\n",
    "\n",
    "- **For each PassengerId in the test set, predict a 0 or 1 value for the Survived variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Logistic regression is not a regression model, it is a classifcation model. Yet, it is closely related to linear regression. \n",
    "\n",
    "This model predicts the probability that a binary variable is 1, by applying a sigmoid function (more precisely, a logistic function) to a linear combination of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "P(y=1|x) &= h_\\theta(x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)} \\equiv \\sigma(\\theta^\\top x),\\\\\\\\\n",
    "P(y=0|x) &= 1 - P(y=1|x) = 1 - h_\\theta(x).\n",
    "\\end{align}$$\n",
    "\n",
    "For a set of training examples with binary labels $\\{ (x_{(i)}, y_{(i)}) : i=1,\\ldots,m\\}$ the following cost function measures how well a given $h_\\theta$ does this:\n",
    "\n",
    "$$ J(\\theta) =\\underset{\\theta, c}{min\\,} C \\frac{1}{2}\\theta^{T} \\theta + \\sum_{i=1}^{m} \\log(1 + \\exp(- y_i (\\theta^T x_i))) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.model_selection as cv\n",
    "import sklearn.grid_search as gs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test datasets with Pandas\n",
    "train = pd.read_csv('Data/titanic_train.csv')\n",
    "test = pd.read_csv('Data/titanic_test.csv')\n",
    "train.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe columns Pclass, Sex, Age, Survived\n",
    "train[train.columns[?]].head()                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will keep only a few fields for this example. \n",
    "\n",
    "- We also convert the `sex` field to a binary variable, so that it can be handled correctly by NumPy and scikit-learn. \n",
    "\n",
    "- Finally, we remove the rows containing `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train[['Sex', 'Age', 'Pclass', 'Survived']].?\n",
    "data['Sex'] = data['Sex'] == ?\n",
    "data = data.dropna()                                                          \n",
    "data.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we convert this `DataFrame` to a NumPy array, so that we can pass it to scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data.astype(np.int32).?\n",
    "X = data_np[?] # Features                                                  \n",
    "y = data_np[?] # Target (survived or not)                                  \n",
    "print X[:5,:]\n",
    "print y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's have a look at the **survival of male and female passengers, as a function of their age**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need few boolean vectors\n",
    "female = (?)  \n",
    "survived = (?)\n",
    "\n",
    "# This vector contains the age of the passengers.\n",
    "age = X[?]\n",
    "\n",
    "# Compute few histograms\n",
    "mybins = np.arange(0, 81, 5)\n",
    "S = {'male': np.histogram(age[survived & ~female], \n",
    "                          bins=mybins)[0],\n",
    "     'female': np.histogram(age[survived & female], \n",
    "                            bins=mybins)[0]}\n",
    "D = {'male': np.histogram(age[~survived & ~female], \n",
    "                          bins=mybins)[0],\n",
    "     'female': np.histogram(age[~survived & female], \n",
    "                            bins=mybins)[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the data\n",
    "bins = mybins[:-1]\n",
    "plt.figure(figsize=(10,3));\n",
    "for i, sex, color in zip((0, 1),\n",
    "                         ('male', 'female'),\n",
    "                         ('#3345d0', '#cc3dc0')):\n",
    "    plt.subplot(121 + i);\n",
    "    plt.bar(bins, S[sex], bottom=D[sex], color=color,\n",
    "            width=5, label='survived');\n",
    "    plt.bar(bins, D[sex], color='k', width=5, label='died');\n",
    "    plt.xlim(0, 80);\n",
    "    plt.grid(None);\n",
    "    plt.title(sex + \" survival\");\n",
    "    plt.xlabel(\"Age (years)\");\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try to train a LogisticRegression classifier. We first need to create a **train and a test dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split X and y into train and test datasets.\n",
    "(X_train, X_test, y_train, y_test) = cv.train_test_split(?, ?, test_size=?)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier / 'Create the model'\n",
    "logreg = lm.LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model (**fit**) and get the predicted values (**predict**) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(?)                  \n",
    "y_predicted = logreg.predict(?)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also use the cross_val_score that computes the **cross-validation score**.\n",
    "\n",
    "- This function uses by default a 3-fold stratified cross-validation procedure, but this can be changed with the cv keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.cross_val_score(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='KNN'></a>\n",
    "# 2. KNN: Recognizing Handwritten Digits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning by Analogy**:\n",
    "\n",
    "\n",
    "> Tell me who your friends are and I’ll tell you who you are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of K-nearest neighbors is as follows: given a new point in the feature space, find the $K$ closest points from the training set and assign the label of the majority of those points. \n",
    "\n",
    "The distance is generally the Euclidean distance, but other distances can be used too.\n",
    "\n",
    "The number $K$ is a hyperparameter of the model. If it is too small, the model will not generalize well (high variance). In particular, it will be highly sensitive to outliers. \n",
    "\n",
    "By contrast, the precision of the model will worsen if $K$ is too large. At the extreme, if $K$ is equal to the total number of points, the model will always predict the exact same value disregarding the input (high bias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the **digits dataset**. This dataset contains hand-written digits that have been manually labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as ds\n",
    "\n",
    "digits = ds.?\n",
    "X = digits.?\n",
    "y = digits.?\n",
    "print((X.min(), X.max()))\n",
    "print(X.shape)\n",
    "print y[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix `X`, each row contains the $8 \\times 8=64$ pixels (in grayscale, values between 0 and 16). The pixels are ordered according to the row-major order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some of the images\n",
    "nrows, ncols = 2, 5\n",
    "print(ncols * nrows)\n",
    "plt.figure(figsize=(8,4));\n",
    "plt.gray()\n",
    "for i in range(ncols * nrows):\n",
    "    ax = plt.subplot(nrows, ncols, i + 1)\n",
    "    ax.matshow(digits.images[i])\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    plt.title(digits.target[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a K-nearest neighbors classifier on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = cv.train_test_split(?, ?, test_size=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.neighbors as nb\n",
    "knc = nb.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.fit(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the score of the trained classifier on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.score(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see if our classifier can recognize a \"hand-written\" digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw a 1                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2));\n",
    "plt.imshow(one, interpolation='none');\n",
    "plt.grid(False);\n",
    "plt.xticks(); plt.yticks();\n",
    "plt.title(\"One\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print one.reshape(1,-1)\n",
    "knc.predict(one.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN on IRIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors as nb\n",
    "\n",
    "iris = ds.?\n",
    "X, y = ?\n",
    "\n",
    "knc = nb.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit the model\n",
    "knc.fit(?)\n",
    "\n",
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "# call the \"predict\" method:\n",
    "result = knc.predict([[3, 5, 4, 2],])\n",
    "\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc.predict_proba([[3, 5, 4, 2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run plot_iris_knn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NaiveBayes'></a>\n",
    "# 3. Naive Bayes: Detecting Insults in Social Commentary (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we see how to handle text data with scikit-learn. \n",
    "\n",
    "\n",
    "- Working with text requires careful preprocessing and feature extraction. \n",
    "\n",
    "\n",
    "- We will learn to recognize whether a comment posted during a public discussion is considered insulting to one of the participants. We will use a labeled dataset from [Impermium](https://en.wikipedia.org/wiki/Impermium), released during a [Kaggle competition](https://www.kaggle.com/c/detecting-insults-in-social-commentary) (Detecting Insults in Social Commentary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes algorithms are **Bayesian methods** based on the **naive assumption of independence between features**. This strong assumption drastically simplifes the computations and leads to very fast yet decent classifers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the Troll dataset from Kaggle competition\n",
    "https://www.kaggle.com/c/detecting-insults-in-social-commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.model_selection as cv\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/troll.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row is a comment. There are three columns: whether the comment is insulting (1) or not (0), the data, and the unicode-encoded contents of the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Insult', 'Comment']].?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the feature matrix $\\mathbf{X}$ and the labels $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df[?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the **feature matrix** from the text is not trivial. Scikit-learn can only work with numerical matrices. \n",
    "\n",
    "**How to convert text into a matrix of numbers?** \n",
    "\n",
    "- A classical solution is to first extract a **vocabulary**: a list of words used throughout the corpus. Then, we can count, for each sample, the frequency of each word. We end up with a **sparse matrix**: a huge matrix containiny mostly zeros. \n",
    "\n",
    "\n",
    "- Frequent words in comments should have a high weight except if they appear in most comments (which means that they are common terms, for example, \"the\" and \"and\" would be fltered out).\n",
    "\n",
    "\n",
    "- **tf–idf** or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how **important a word is to a document** in a collection or corpus.\n",
    "\n",
    "\n",
    "- TF: **Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "     > TF($t$) = (Number of times term $t$ appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "\n",
    "- IDF: **Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "     > IDF($t$) = log_e(Total number of documents / Number of documents with term $t$ in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(df[?])\n",
    "print(X.shape)\n",
    "print X.nnz\n",
    "print X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3947 comments and 16469 different words. Let's estimate the sparsity of this feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Each sample has ~{0:.2f}% non-zero features.\".format(100 * X.nnz / float(X.shape[0] * X.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = cv.train_test_split(?, ?, test_size=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use a **Bernoulli Naive Bayes classifier** with a grid search on the smoothing parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = cv.GridSearchCV(nb.?, param_grid={'alpha':np.logspace(-2., 2., 50)})\n",
    "bnb.fit(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the performance of this classifier on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.score(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a look at the words corresponding to the largest coefficients (the words we find frequently in insulting comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the words corresponding to each feature\n",
    "names = np.asarray(tf.get_feature_names())\n",
    "\n",
    "# Next, we display the 50 words with the largest coefficients\n",
    "print(','.join(names[np.argsort(bnb.best_estimator_.coef_[0,:])[::-1][:50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's test our estimator on a few test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bnb.?(tf.transform([\n",
    "    \"I totally agree with you.\",\n",
    "    \"You are so stupid.\",\n",
    "    \"I love you.\"\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bnb.?(tf.transform([\n",
    "    \"I totally agree with you.\",\n",
    "    \"You are so stupid.\",\n",
    "    \"I love you.\"\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SVM_C'></a>\n",
    "# 4. Support Vector Machines: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **binary-class linear SVC** tries to find a **hyperplane** that **best separates** the **two sets** of points (grouped according to their labels). There is also the constraint that this separating hyperplane needs to be **as far as possible from the points**. \n",
    "\n",
    "\n",
    "- XOR is known as being a nonlinearly separable operation, so a hyperplane may not exist.\n",
    "\n",
    "\n",
    "- The SVM classes in scikit-learn has a $C$ hyperparameter. This hyperparameter trades off misclassifcation of training examples against simplicity of the decision surface. A low $C$ value makes the decision surface smooth, while a high $C$ value aims at classifying all training examples correctly. This hyperparameter can be chosen with cross-validation and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\min_{w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i $$\n",
    "\n",
    "                               subject to\n",
    " \n",
    "$$  y_i(w^{T} \\phi(x_i) + b) \\geq 1 - \\zeta_i,\\\\\n",
    "    \\zeta_i \\geq 0, i = 1, ..., n $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The linear SVC can also be extended to **multiclass** problems. The multiclass SVC is directly implemented in scikit-learn.\n",
    "\n",
    "\n",
    "- The **nonlinear SVC** works by considering a nonlinear transformation $\\phi(x)$ from the original space into a higher dimensional space. This nonlinear transformation can increase the linear separability of the classes. In practice, all dot products are replaced by the **kernel** $k(x,x^{'}) = \\phi(x) \\times \\phi(x^{'})$.\n",
    "\n",
    "\n",
    "- There are several widely-used nonlinear kernels. By default, SVC uses **Gaussian Radial Basis Functions (RBF)** having the kernel:\n",
    "\n",
    "$$ k(x,x^{'}) = exp(-\\gamma \\| x - x^{'} \\|^{2}) $$\n",
    "\n",
    "\n",
    "- Here, $\\gamma$ is a **hyperparameter** of the model that can be chosen with **grid search and cross-validation**. The $\\phi$ function does not need to be computed explicitly. This is the **kernel trick**; it suffces to know the kernel $k(x,x^{'})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.model_selection as cv\n",
    "import sklearn.grid_search as gs\n",
    "import sklearn.svm as svm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We generate 2D points and assign a binary label according to a linear operation on the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.?\n",
    "y = (X[:, 0] + X[:, 1]) > 1 \n",
    "\n",
    "X[?], y[?]                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a linear Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = svm.?\n",
    "est.fit(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the classification results with the linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_decisionfun import *\n",
    "plot_decision_function(est, X, y)                                   \n",
    "plt.title(\"Linearly separable, linear SVC\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear SVC tried to separate the points with a line and it did a good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now modify the labels with a **XOR** function. \n",
    "\n",
    "\n",
    "- A point's label is 1 if the coordinates have different signs. This classification is not linearly separable. Therefore, a linear SVC fails completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.?(X[:, 0] > 0, X[:, 1] > 0)                                              \n",
    "\n",
    "# Train the classifier\n",
    "est = gs.?(svm.LinearSVC(), \n",
    "                      {'C': np.logspace(-3., 3., 10)});\n",
    "est.fit(?)\n",
    "\n",
    "print(\"Score: {0:.1f}\".format(cv.cross_val_score(?).mean()))                       \n",
    "\n",
    "# Plot the decision function\n",
    "plot_decision_function(est, X, y)                                                          \n",
    "plt.title(\"XOR, linear SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is possible to use **non-linear SVCs** by using non-linear **kernels**. \n",
    "\n",
    "\n",
    "- Kernels specify a **non-linear transformation** of the points into a **higher-dimensional space**. Transformed points in this space are assumed to be more linearly separable, although they are not necessarily in the original space. \n",
    "\n",
    "\n",
    "- By default, the `SVC` classifier in scikit-learn uses the **Radial Basis Function** (RBF) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "est = gs.GridSearchCV(svm.?\n",
    "                      {'C': np.logspace(-3., 3., 10),\n",
    "                       'gamma': np.logspace(-3., 3., 10)});\n",
    "est.fit(?)\n",
    "print(\"Score: {0:.3f}\".format(\n",
    "      cv.cross_val_score(?).mean()))\n",
    "\n",
    "plot_decision_function(est.best_estimator_, X, y)\n",
    "plt.title(\"XOR, non-linear SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the non-linear SVC does a good job at classifying these non-linearly separable points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVM on IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iris = datasets.?\n",
    "X = iris.data[?]\n",
    "y = iris.?\n",
    "\n",
    "# Plot resulting Support Vector boundaries with original data\n",
    "# Create fake input data for prediction that we will use for plotting\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = (x_max / x_min)/100\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X_plot = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Create the SVC model object\n",
    "C = 1.0 # SVM regularization parameter\n",
    "# For multi-class, we use One-v/s-Rest here\n",
    "svc = svm.SVC(kernel=?, C=C, decision_function_shape=?).fit(?)\n",
    "Z = svc.predict(?)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Set1, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.cool)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.title('SVC with linear kernel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's do **parameter tuning**\n",
    "- Use **5-fold cross validation** to perform **grid search** to calculate **optimal hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# shuffle the dataset\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = cv.train_test_split(?, ?, test_size=?)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "parameters = [{'kernel': ['rbf'],\n",
    "               'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n",
    "                'C': [1, 10, 100, 1000]},\n",
    "              {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "print(\"# Tuning hyper-parameters\")\n",
    "print\n",
    "\n",
    "clf = ?(svm.?(decision_function_shape=?), parameters, cv=5)                 \n",
    "clf.fit(?)\n",
    "\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print\n",
    "print(clf.best_params_)\n",
    "print\n",
    "print(\"Grid scores on training set:\")\n",
    "print\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detailed classification report:\")\n",
    "print\n",
    "print(\"The model is trained on the training set.\")\n",
    "print(\"The scores are computed on the testing set.\")\n",
    "print\n",
    "y_true, y_pred = y_test, clf.predict(?)\n",
    "print(classification_report(?))\n",
    "print\n",
    "# The support is the number of occurrences of each class in y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from accuracy, three major metrics to understand the task for classification are: precision, recall and f1-score.\n",
    "\n",
    "**Precision**: The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "**Recall**: The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F1-Score**: It can be interpreted as a weighted harmonic mean of the precision and recall, where an f1-score reaches its best value at 1 and worst score at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SVM_R'></a>\n",
    "# 5. Support Vector Machines: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First generate a random dataset on which we want to generate a regression model. \n",
    "\n",
    "\n",
    "- In order to have a good visualization of our results, it would be best to use a single feature as an example. \n",
    "\n",
    "\n",
    "- In order to study effect of non-linear models, we will be generating our data from the **sin() function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sort(5 * np.random.rand(200, 1), axis=0)\n",
    "y = np.sin(X).ravel()                                              \n",
    "\n",
    "print y[:10]\n",
    "\n",
    "# Adding noise\n",
    "y[?] += 3 * (0.5 - np.random.rand(40))                           # every 5th instance is corrupted\n",
    "print y[?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_rbf = SVR(kernel=?, C=1e3, gamma=0.1)                      \n",
    "\n",
    "svr_lin = SVR(kernel=?, C=1e3)\n",
    "\n",
    "svr_poly = SVR(kernel=?, C=1e3, degree=3)\n",
    "\n",
    "y_rbf = svr_rbf.fit(?).predict(?)\n",
    "y_lin = svr_lin.fit(?).predict(?)\n",
    "y_poly = svr_poly.fit(?).predict(?) \n",
    "\n",
    "lw = 2\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')\n",
    "plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Further parameter tuning can be easily done just as above in SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DecTree'></a>\n",
    "# 6. Decision Tree Classifier: Evaluating Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_data = digits.?\n",
    "digits_labels = digits.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the images\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_test, dl_train, dl_test = cv.train_test_split(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our data\n",
    "clftree = DecisionTreeClassifier()\n",
    "clftree.fit(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "clftree.predict(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test set labels\n",
    "dl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, cross-validate on test data\n",
    "cv.cross_val_score(?, ?, ?, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DR_PCA'></a>\n",
    "# 7. Dimensionality Reduction: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "PCA is based on a matrix decomposition called Singular Value Decomposition (SVD):\n",
    "\n",
    "$$ X = U \\Sigma V^{T} $$\n",
    "\n",
    "$X$ is the $(N,D)$ data matrix, $U$ and $V$ are orthogonal matrices ($ U U^T = I$), and $\\Sigma$ is a $(N,D)$ diagonal matrix.\n",
    "\n",
    "PCA transforms $X$ into $X^{'}$ defined by:\n",
    "\n",
    "$$ X^{'} = XV = U \\Sigma $$\n",
    "\n",
    "The diagonal elements of $\\Sigma$ are the singular values of $X$. By convention, they are generally sorted in descending order. \n",
    "\n",
    "The columns of $U$ are orthonormal vectors called the left singular vectors of $X$. Therefore, the columns of $X^{'}$ are the left singular vectors multiplied by the singular values.\n",
    "\n",
    "In the end, PCA converts the initial set of observations, which are made of possibly correlated variables, into vectors of linearly uncorrelated variables called **principal components**.\n",
    "\n",
    "The first new feature (or first component) is a transformation of all original features such that the dispersion (variance) of the data points is the highest in that direction. In the subsequent principal components, the variance is decreasing. In other words, PCA gives us an alternative representation of our data where the new features are sorted according to how much they\n",
    "account for the variability of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "print X\n",
    "\n",
    "pca = PCA(n_components=?)                           \n",
    "\n",
    "pca.fit(?)\n",
    "\n",
    "print pca.components_ # Principal axes in feature space, representing the directions of maximum variance in the data\n",
    "\n",
    "print(pca.explained_variance_ratio_)# Percentage of variance explained by each of the selected components\n",
    "\n",
    "X_new = pca.?(?)\n",
    "print X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = ds.load_iris()\n",
    "X = iris.?\n",
    "y = iris.?\n",
    "print(X.shape)\n",
    "\n",
    "plt.figure(figsize=(6,3));\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=30, cmap=plt.cm.rainbow);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply PCA on the dataset to get the transformed matrix. This operation can be done in a single line with scikit-learn: we instantiate a PCA model, and call the fit_transform method. This function computes the principal components first, and projects the data then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as dec\n",
    "pca = dec.?\n",
    "\n",
    "X_bis = pca.?\n",
    "\n",
    "print pca.components_\n",
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now display the same dataset, but in a new coordinate system (or equivalently, a linearly transformed version of the initial dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_bis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bis[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3));\n",
    "plt.scatter(X_bis[:,0], X_bis[:,1], c=y, s=30, cmap=plt.cm.rainbow);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Points belonging to the same classes are now grouped together, even though the PCA estimator dit not use the labels. \n",
    "- The PCA was able to find a projection maximizing the variance, which corresponds here to a projection where the classes are well separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "Extends conventional principal component analysis (PCA) to a high dimensional feature space using the **kernel trick**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Kernel PCA\n",
    "kpca = dec.KernelPCA(kernel=?)\n",
    "\n",
    "X_ter = kpca.fit_transform(?)\n",
    "\n",
    "plt.figure(figsize=(6,3));\n",
    "plt.scatter(X_ter[:,0], X_ter[:,1], c=y, s=30, cmap=plt.cm.rainbow);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('Data/wine.data', header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "df_wine.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data into 70% training and 30% test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_wine.iloc[:, 1:].?, df_wine.iloc[:, 0].?\n",
    "\n",
    "X_train, X_test, y_train, y_test = cv.train_test_split(?, ?, test_size=?, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(?)\n",
    "\n",
    "X_test_std = sc.transform(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = ?\n",
    "\n",
    "X_train_pca = pca.fit_transform(?)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align='center',label='individual explained variance')\n",
    "plt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=?)\n",
    "\n",
    "X_train_pca = pca.fit_transform(?)\n",
    "\n",
    "X_test_pca = pca.transform(?)\n",
    "\n",
    "plt.scatter(X_train_pca[?], X_train_pca[?])\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train Logistic Regression classifier using the first 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from plot_decisionregion import *\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(?)\n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)                           \n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)            \n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example of Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X, y = ds.?(n_samples=100) \n",
    "\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's try with PCA\n",
    "pca = PCA(n_components=?)\n",
    "X_pca = pca.fit_transform(?)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n",
    "\n",
    "ax[0].scatter(X_pca[y == 0, 0], X_pca[y == 0, 1],\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_pca[y == 1, 0], X_pca[y == 1, 1],\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax[1].scatter(X_pca[y == 0, 0], np.zeros((50, 1)) + 0.02,\n",
    "              color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_pca[y == 1, 0], np.zeros((50, 1)) - 0.02,\n",
    "              color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1, 1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA didn't do a good job; let's try kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=?, kernel=?, gamma=15)\n",
    "X_kpca = kpca.fit_transform(?)                      \n",
    "\n",
    "plt.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1],\n",
    "            color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1],\n",
    "            color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clustering'></a>\n",
    "# 8. Clustering: Detecting Hidden Structures in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we'll explore **K Means Clustering**, which is an unsupervised clustering technique.\n",
    "\n",
    "\n",
    "- K Means is an algorithm for **unsupervised clustering**: that is, finding clusters in data based on the data attributes alone (not the labels).\n",
    "\n",
    "\n",
    "- K Means is a relatively easy-to-understand algorithm.  It searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = ?(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "plt.scatter(X[?], X[?], s=50)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By eye, it is relatively easy to pick out the four clusters. \n",
    "\n",
    "\n",
    "- If we were to perform an exhaustive search for the different segmentations of the data, however, the search space would be exponential in the number of points. \n",
    "\n",
    "\n",
    "- Fortunately, there is a well-known **Expectation Maximization (EM)** procedure which scikit-learn implements, so that KMeans can be solved relatively quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "est = ?\n",
    "\n",
    "est.fit(?)\n",
    "\n",
    "y_kmeans = est.predict(?)\n",
    "\n",
    "plt.scatter(X[?], X[?], c=?, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm identifies the four clusters of points in a manner very similar to what we would do by eye!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The K-Means Algorithm: Expectation Maximization\n",
    "\n",
    "K-Means is an example of an algorithm which uses an *Expectation-Maximization* approach to arrive at the solution.\n",
    "*Expectation-Maximization* is a two-step approach which works as follows:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "\n",
    "2. Repeat until converged\n",
    "\n",
    "   A. Assign points to the nearest cluster center\n",
    "   \n",
    "   B. Set the cluster centers to the mean \n",
    "   \n",
    "Let's quickly visualize this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InteractiveKMeans import *\n",
    "plot_kmeans_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm will (often) converge to the optimal cluster centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of KMeans to Digits dataset\n",
    "\n",
    "Here we'll use KMeans to automatically cluster the data in 64 dimensions, and then look at the cluster centers to see what the algorithm has found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=?)\n",
    "\n",
    "clusters = est.fit_predict(?)\n",
    "\n",
    "est.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see ten clusters in 64 dimensions. Let's visualize each of these cluster centers to see what they represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    ax.imshow(est.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *even without the labels*, KMeans is able to find clusters whose means are recognizable digits (sorry to number 8)!\n",
    "\n",
    "The cluster labels are permuted; let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(?))\n",
    "\n",
    "plt.imshow(confusion_matrix(?),\n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Estimation: Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now explore **Gaussian Mixture Models**, which is an unsupervised clustering & density estimation technique.\n",
    "\n",
    "\n",
    "- We previously saw an example of K-Means, which is a clustering algorithm which is most often fit using an expectation-maximization approach.\n",
    "\n",
    "\n",
    "- Here we'll consider an extension to this which is suitable for both **clustering** and **density estimation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Motivating GMM: Weaknesses of K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y_true = ?(n_samples=400, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "\n",
    "X = X[:, ::-1] # flip axes for better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with K Means Labels\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(?, random_state=0)\n",
    "\n",
    "labels = kmeans.?\n",
    "\n",
    "plt.scatter(X[?], X[?], c=labels, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From an intuitive standpoint, we might expect that the clustering assignment for some points is more certain than others: for example, there appears to be a very slight overlap between the two middle clusters, such that we might not have complete confidence in the cluster assigment of points between them.\n",
    "\n",
    "\n",
    "\n",
    "- Unfortunately, the *K*-means model has no intrinsic measure of probability or uncertainty of cluster assignments.\n",
    "\n",
    "\n",
    "\n",
    "- For this, we must think about generalizing the model.\n",
    "\n",
    "\n",
    "\n",
    "- One way to think about the *K*-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster.\n",
    "\n",
    "\n",
    "\n",
    "- This radius acts as a **hard cutoff for cluster assignment** within the training set: any point outside this circle is not considered a member of the cluster.\n",
    "\n",
    "\n",
    "\n",
    "- We can visualize this cluster model with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GMM_KM import *\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "plot_kmeans(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An important observation for *K*-means is that these cluster models **must be circular**: *K*-means has no built-in way of accounting for oblong or elliptical clusters.\n",
    "\n",
    "\n",
    "- So, for example, if we take the same data and transform it, the cluster assignments end up becoming confused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(?, rng.randn(2, 2)) # data multipled with Gaussian noise     \n",
    "\n",
    "kmeans = KMeans(n_clusters=?)\n",
    "plot_kmeans(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By eye, we recognize that these transformed clusters are non-circular, and thus circular clusters would be a poor fit.\n",
    "\n",
    "\n",
    "- $K$-means is not flexible enough to account for this, and tries to force-fit the data into four circular clusters.\n",
    "\n",
    "\n",
    "- This results in a mixing of cluster assignments where the resulting circles overlap: see especially the bottom-right of this plot.\n",
    "\n",
    "\n",
    "- The disadvantages of $K$-means: its lack of flexibility in cluster shape and lack of probabilistic cluster assignment, means that for many datasets (especially low-dimensional datasets) it may not perform well.\n",
    "\n",
    "\n",
    "- Time for Gaussian Mixture Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing E–M: Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=?).?\n",
    "labels = gmm.?\n",
    "plt.scatter(X[?], X[?], c=labels, s=40, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since GMM is a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the ``predict_proba`` method.\n",
    "\n",
    "\n",
    "- This returns a matrix of size ``[n_samples, n_clusters]`` which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = gmm.predict_proba(?)\n",
    "print(probs[:5].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, a Gaussian mixture model is very similar to $K$-means: it uses an **Expectation–Maximization** approach which qualitatively does the following:\n",
    "\n",
    "1. Choose starting guesses for the location and shape\n",
    "\n",
    "2. Repeat until converged:\n",
    "\n",
    "   1. **E-step**: for each point, find weights encoding the probability of membership in each cluster\n",
    "   2. **M-step**: for each cluster, update its location, normalization, and shape based on *all* data points, making use of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=?)\n",
    "plot_gmm(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=?, covariance_type=?)\n",
    "plot_gmm(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gmm.weights_ \n",
    "print gmm.means_ \n",
    "print gmm.covariances_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----END----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
